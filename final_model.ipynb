{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f065cd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a97c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neural_network import MLPClassifier # The \"Scikit-Learn Neural Net\"\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8073e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "sample_sub = pd.read_csv(f\"data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe540e",
   "metadata": {},
   "source": [
    "# Loading preprocessed Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f23ecb6",
   "metadata": {},
   "source": [
    "### Training Embeddings\n",
    "- **Metric embedding:** Representation of model-generated metrics for a response  \n",
    "- **Response embedding:** Embedding of the generated response  \n",
    "- **User prompt embedding:** Representation of the input query  \n",
    "- **System prompt embedding:** Embedding of the system instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d522a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Load embeddings and targets\n",
    "# ============================\n",
    "EMB  = \"./data\"\n",
    "y = np.load('data/train_scores.npy')\n",
    "\n",
    "# Load ALL embeddings (System included)\n",
    "train_metric = np.load(f\"{EMB}/train_metric_embedding.npy\").astype(np.float32)\n",
    "train_resp   = np.load(f\"{EMB}/train_response_embedding.npy\").astype(np.float32)\n",
    "train_user   = np.load(f\"{EMB}/train_user_prompt_embedding.npy\").astype(np.float32)\n",
    "train_sys = np.load(f\"{EMB}/train_system_prompt_embedding.npy\").astype(np.float32)\n",
    "\n",
    "test_metric = np.load(f\"{EMB}/test_metric_embedding.npy\").astype(np.float32)\n",
    "test_resp   = np.load(f\"{EMB}/test_response_embedding.npy\").astype(np.float32)\n",
    "test_user   = np.load(f\"{EMB}/test_user_prompt_embedding.npy\").astype(np.float32)\n",
    "test_sys = np.load(f\"{EMB}/test_system_prompt_embedding.npy\").astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b84e22",
   "metadata": {},
   "source": [
    "### Target Labels\n",
    "`train_scores.npy` contains the ground-truth numerical score for each training data point.\n",
    "\n",
    "After loading, we horizontally concatenate system + user + response embeddings to create a unified text embedding for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c0b5f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Vector Shape: (5000, 2304)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. PREPARE FEATURES ---\n",
    "# Concat System + User + Response\n",
    "X_text_train = np.hstack([train_sys, train_user, train_resp])\n",
    "X_text_test  = np.hstack([test_sys,  test_user,  test_resp])\n",
    "X_metric_train = train_metric\n",
    "X_metric_test  = test_metric\n",
    "\n",
    "print(f\"Text Vector Shape: {X_text_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40a3cf",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Feature engineering is the core of the model’s predictive power.  \n",
    "We build three new meaningful features:\n",
    "\n",
    "---\n",
    "\n",
    "## Isolation Forest Outlier Score\n",
    "\n",
    "**Goal:** Identify how unusual a response is within the text embedding space.\n",
    "\n",
    "Steps:\n",
    "1. Train an `IsolationForest` model on the combined text embeddings.\n",
    "2. Obtain anomaly scores using `.decision_function()`.\n",
    "3. Scale the values using `MinMaxScaler` so S1 ranges between 0 and 1.\n",
    "\n",
    "A lower score means \"more unusual,\" while a higher score indicates a more typical or well-structured response.\n",
    "\n",
    "---\n",
    "\n",
    "## Match Probability using MLP\n",
    "\n",
    "**Motivation:**  \n",
    "Rewards where metric embedding and response embedding logically agree.\n",
    "\n",
    "### Positive Samples:\n",
    "Pairs of (metric_embedding, response_embedding) from the same instance.\n",
    "\n",
    "### Negative Samples:\n",
    "We shuffle the response embeddings to create mismatched pairs:\n",
    "- Realistic \"good\" pairs get label = 1  \n",
    "- Synthetic mismatches get label = 0\n",
    "\n",
    "### Model:\n",
    "\n",
    "We train a two-layer `MLPClassifier`:\n",
    "\n",
    "```\n",
    "Input → Dense(512) → Dense(256) → Probability\n",
    "```\n",
    "\n",
    "This creates a latent projection that learns whether the metric embedding \"belongs\" to the text embedding.\n",
    "\n",
    "The output is the predicted probability that a pair is consistent.\n",
    "\n",
    "---\n",
    "\n",
    "##  Cosine Similarity\n",
    "\n",
    "This captures alignment between the user prompt and generated response.\n",
    "\n",
    "We compute:\n",
    "\n",
    "```\n",
    "cosine_similarity(user_embedding, response_embedding)\n",
    "```\n",
    "\n",
    "This value is again scaled to 0–1 using MinMaxScaler.\n",
    "\n",
    "Higher score means the response closely matches the intent of the user query.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "The three features represent:\n",
    "\n",
    "| Feature | What it measures |\n",
    "|---|---|\n",
    "| S1 | Outlier likelihood of the generated response |\n",
    "| S2 | Whether the metric and response embeddings logically match |\n",
    "| S3 | Semantic relevance between user query and model response |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41623480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting S1 (Isolation Forest)...\n",
      "Fitting S2 (MLP Matcher)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rishi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Define New Features ---\n",
    "\n",
    "# Anomaly Detection (Isolation Forest)\n",
    "# Detects outliers in the text embedding space.\n",
    "print(\"Fitting S1 (Isolation Forest)...\")\n",
    "iso = IsolationForest(n_estimators=100, contamination='auto', n_jobs=-1, random_state=42)\n",
    "iso.fit(X_text_train)\n",
    "\n",
    "# MLPClassifier\n",
    "# This allows the model to mathematically project the Metric and Text to see if they match.\n",
    "print(\"Fitting S2 (MLP Matcher)...\")\n",
    "\n",
    "# 1. Positives (Real Pairs)\n",
    "X_pos = np.hstack([X_metric_train, X_text_train])\n",
    "y_pos = np.ones(len(X_pos))\n",
    "\n",
    "# 2. Negatives (Shuffled Pairs)\n",
    "# We use 2x negatives to make the model really strict about what a \"Match\" is.\n",
    "rng = np.random.RandomState(42)\n",
    "idx_shuf1 = rng.permutation(len(X_text_train))\n",
    "idx_shuf2 = rng.permutation(len(X_text_train))\n",
    "\n",
    "X_neg1 = np.hstack([X_metric_train, X_text_train[idx_shuf1]])\n",
    "X_neg2 = np.hstack([X_metric_train, X_text_train[idx_shuf2]])\n",
    "y_neg = np.zeros(len(X_neg1) + len(X_neg2))\n",
    "\n",
    "X_matcher = np.vstack([X_pos, X_neg1, X_neg2])\n",
    "y_matcher = np.concatenate([y_pos, y_neg])\n",
    "\n",
    "# MLP: Hidden layers project the high-dim embeddings. \n",
    "# This is fast on CPU for this dataset size (~15k rows total).\n",
    "matcher = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 256), # Layers to compress info\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size=256,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=50, # 50 epochs is usually enough\n",
    "    random_state=42\n",
    ")\n",
    "matcher.fit(X_matcher, y_matcher)\n",
    "\n",
    "# Cosine Similarity \n",
    "def get_s3(u, r):\n",
    "    num = np.sum(u * r, axis=1)\n",
    "    den = np.linalg.norm(u, axis=1) * np.linalg.norm(r, axis=1)\n",
    "    return num / (den + 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d652a1",
   "metadata": {},
   "source": [
    "# Synthetic Data Augmentation\n",
    "\n",
    "We generate synthetic negative samples:\n",
    "\n",
    "- We use the real metric embeddings untouched.\n",
    "- We randomly shuffle:\n",
    "  - user embeddings\n",
    "  - response embeddings\n",
    "\n",
    "This creates inputs that should correspond to **low-quality output**.\n",
    "\n",
    "## Synthetic Scores\n",
    "\n",
    "We assign synthetic labels uniformly in the range:\n",
    "\n",
    "```\n",
    "0.0 – 3.5\n",
    "```\n",
    "\n",
    "This makes the regression model strongly aware of what bad responses look like statistically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. AUGMENTATION ---\n",
    "\n",
    "# Augment Training Data for Regressor\n",
    "# We mix real data with synthetic \"bad\" data so LightGBM learns to predict low scores.\n",
    "n_aug = len(y)\n",
    "idx_aug = rng.permutation(len(y))\n",
    "\n",
    "# Synthetic Data\n",
    "aug_metric = X_metric_train\n",
    "aug_text   = X_text_train[idx_aug]\n",
    "aug_user   = train_user[idx_aug]\n",
    "aug_resp   = train_resp[idx_aug]\n",
    "# Synthetic Scores: Uniform 0.0 - 3.5 (Strictly low)\n",
    "aug_y      = rng.uniform(0.0, 3.5, size=n_aug)\n",
    "\n",
    "# Combine\n",
    "X_text_total = np.vstack([X_text_train, aug_text])\n",
    "X_metric_total = np.vstack([X_metric_train, aug_metric])\n",
    "train_user_total = np.vstack([train_user, aug_user])\n",
    "train_resp_total = np.vstack([train_resp, aug_resp])\n",
    "y_total = np.concatenate([y, aug_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMPUTE FEATURES ---\n",
    "\n",
    "s1_raw_tr = iso.decision_function(X_text_total)\n",
    "s1_raw_te = iso.decision_function(X_text_test)\n",
    "sc1 = MinMaxScaler()\n",
    "S1_train = sc1.fit_transform(s1_raw_tr.reshape(-1,1)).flatten()\n",
    "S1_test  = sc1.transform(s1_raw_te.reshape(-1,1)).flatten()\n",
    "\n",
    "X_concat_tr = np.hstack([X_metric_total, X_text_total])\n",
    "X_concat_te = np.hstack([X_metric_test, X_text_test])\n",
    "S2_train = matcher.predict_proba(X_concat_tr)[:, 1]\n",
    "S2_test  = matcher.predict_proba(X_concat_te)[:, 1]\n",
    "\n",
    "s3_raw_tr = get_s3(train_user_total, train_resp_total)\n",
    "s3_raw_te = get_s3(test_user, test_resp)\n",
    "sc3 = MinMaxScaler()\n",
    "S3_train = sc3.fit_transform(s3_raw_tr.reshape(-1,1)).flatten()\n",
    "S3_test  = sc3.transform(s3_raw_te.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9abf36f",
   "metadata": {},
   "source": [
    "# Final Training Feature Construction\n",
    "\n",
    "After computing the S1, S2, and S3 signals, we assemble our final model input.\n",
    "\n",
    "To give LightGBM more separation power, we engineer additional interaction features:\n",
    "\n",
    "| Feature | Formula | Meaning |\n",
    "|---|---|---|\n",
    "| S1xS2 | S1 * S2 | Alignment of anomaly detection + metric matching |\n",
    "| S2xS3 | S2 * S3 | Logical agreement + semantic alignment |\n",
    "| S1xS3 | S1 * S3 | Outlier vs prompt consistency |\n",
    "| All | S1 * S2 * S3 | High when all three indicators agree |\n",
    "\n",
    "The resulting dataframe is small but dense and expressive, ideal for boosting models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fbb34d",
   "metadata": {},
   "source": [
    "# LightGBM Regression Training\n",
    "\n",
    "We use LightGBM to predict the final score.\n",
    "\n",
    "## Training Setup\n",
    "\n",
    "```\n",
    "objective: regression\n",
    "metric: rmse\n",
    "num_leaves: 63\n",
    "learning_rate: 0.05\n",
    "min_data_in_leaf: 10\n",
    "```\n",
    "\n",
    "These parameters are tuned to:\n",
    "\n",
    "- Allow non-linear splits\n",
    "- Recover multi-peaked score distributions\n",
    "- Capture the structure of the engineered features\n",
    "\n",
    "## Sample Weights\n",
    "\n",
    "To avoid LightGBM favoring common score ranges, we compute:\n",
    "\n",
    "```\n",
    "class_weight = 1 / (frequency of rounded score bucket)\n",
    "```\n",
    "\n",
    "This balances the regression training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca570a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n"
     ]
    }
   ],
   "source": [
    "# --- 6. FINAL REGRESSION ---\n",
    "def get_feats(s1, s2, s3):\n",
    "    df = pd.DataFrame({\"S1\": s1, \"S2\": s2, \"S3\": s3})\n",
    "    # Strong interactions to isolate the \"Perfect\" cluster\n",
    "    df[\"S1xS2\"] = df.S1 * df.S2\n",
    "    df[\"S2xS3\"] = df.S2 * df.S3\n",
    "    df[\"S1xS3\"] = df.S1 * df.S3\n",
    "    df[\"All\"]   = df.S1 * df.S2 * df.S3\n",
    "    return df\n",
    "\n",
    "df_train = get_feats(S1_train, S2_train, S3_train)\n",
    "df_test  = get_feats(S1_test, S2_test, S3_test)\n",
    "\n",
    "# Sample Weights\n",
    "counts = pd.Series(np.round(y_total)).value_counts()\n",
    "weights = pd.Series(np.round(y_total)).map(lambda x: 1.0 / counts.get(x, 1.0)).values\n",
    "\n",
    "print(\"Training LightGBM...\")\n",
    "dtrain = lgb.Dataset(df_train, label=y_total, weight=weights)\n",
    "\n",
    "# Parameters optimized to separate clusters rather than smooth the mean\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.05, \n",
    "    \"num_leaves\": 63,        # Higher leaves = more complex splits (good for bimodal)\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"min_data_in_leaf\": 10,  # Allow smaller clusters (peaks)\n",
    "    \"seed\": 42,\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "model = lgb.train(params, dtrain, num_boost_round=1200)\n",
    "pred_test = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3aa411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission Saved.\n",
      "count    3638.000000\n",
      "mean        5.596733\n",
      "std         3.398753\n",
      "min         0.101029\n",
      "25%         1.870185\n",
      "50%         6.764587\n",
      "75%         8.875907\n",
      "max        10.000000\n",
      "Name: score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 8. EXPORT\n",
    "id_col = sample_sub.columns[0]\n",
    "final_df = pd.DataFrame({id_col: sample_sub[id_col], 'score': pred_test})\n",
    "\n",
    "# Optional: Clip to valid range just to be safe (0-10), but no shifting\n",
    "final_df['score'] = np.clip(final_df['score'], 0, 10)\n",
    "\n",
    "final_df.to_csv(\"stacking_submission_new.csv\", index=False)\n",
    "\n",
    "print(\"Submission Saved.\")\n",
    "print(final_df['score'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fffd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean (0–6): 2.23646495159269\n",
      "Mean (6–10): 8.690876978928015\n"
     ]
    }
   ],
   "source": [
    "sub = pd.read_csv('stacking_submission_new.csv')\n",
    "low_mean = sub[sub['score'] <= 6]['score'].mean()\n",
    "high_mean = sub[sub['score'] > 6]['score'].mean()\n",
    "\n",
    "print(\"Mean (0–6):\", low_mean)\n",
    "print(\"Mean (6–10):\", high_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edd95fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sub = pd.read_csv(\"stacking_submission_new.csv\")\n",
    "\n",
    "low_mean  = sub[sub['score'] <= 6]['score'].mean()\n",
    "high_mean = sub[sub['score'] > 6]['score'].mean()\n",
    "\n",
    "delta_low  = 3 - low_mean\n",
    "delta_high = 9 - high_mean\n",
    "\n",
    "# Apply shifts directly to score\n",
    "sub.loc[sub['score'] <= 6, 'score'] += delta_low\n",
    "sub.loc[sub['score'] >  6, 'score'] += delta_high\n",
    "\n",
    "sub['score'] = sub['score'].clip(upper=10)\n",
    "\n",
    "# Save output\n",
    "sub.to_csv(\"stacking_submission_shifted_10ub.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
